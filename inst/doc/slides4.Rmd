---
title: "chapter 4: Apache Spark"
author: Colin Gillespie
date: "`r Sys.Date()`"
output: ioslides_presentation
css: left.css
---

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Chapter 4 Slides}
-->


# 4.1 What is Apache Spark?
## 4.1 What is Apache Spark?

  * Apache Spark is a computing platform whose goal is to make the analysis of large datasets fast.
  * Spark extends the __MapReduce__ paradigm for parallel computing to support a wide range of operations. 
  * A key feature of Spark is that it can run complex computational tasks both in-memory and on disk.

## What is Apache Spark?

  * The Spark project contains multiple tightly integrated components. 
  * This closely coupled design means that improvements in one part of the Spark engine are automatically used by other components. 
  * Another benefit of the tight coupling between Spark's components is that there is a single system to maintain, which can be crucial for large organisations.

## The Spark stack

  * Spark Core: The Core contains the basic functionality of Spark, such as memory management, fault recovery, and interacting with storage systems. Spark Core provides APIs that enable the other components to access these collections.
  * Spark SQL: This library provides an SQL interface for interacting with databases.
  * Spark Streaming: Functionality designed to ease the management of data collected in real-time.

## The Spark stack

  * MLlib: A scalable machine learning library.
  * GraphX: A relatively new component in Spark for representing and analysing phenomena that can be represented as graphs, such as person-to-person links in social media.
  * Cluster Managers.
  * Third party libraries: Like R and Python, developers are encouraged to
  extend Spark. Over 100 additional libraries have been contributed so far,
  each of which can be rated by the community\sidenote{\url{http://spark-packages.org/}}. 

## Does anyone use it?

  * Spark is rapidly becoming the a key component on analysing data sets that have to be __distributed__ across multiple computers. 
  * To get an idea of Spark's popularity, browse the Powered by Spark webpage (see notes)
    

## What about Hadoop?

  * The `rmr2` package allows the R to use Hadoop MapReduce. 
  * However, development on this package has slowed. 
  * The author notes that the lack of activity on `rmr2`  is due to two reasons. 
    * Package maturity. 
    * The general shift away from Hadoop MapReduce towards Spark.




## R and Spark: `SparkR`

  * `SparkR` was released as a separated component of Spark in 2014. 
    * In June 2015, merged into the main Spark distribution. 
  * They are still in the process of deciding on the API of `SparkR`. 

    1. The full functionality of Spark is __not__ yet available via `SparkR`.
    1. If you find code online, it's likely not to work since it uses the old `SparkR` package.

## R and Spark: `Sparklyr`

  * Made by RStudio
  * First version on CRAN last week!
  * Bleeding edge

  * In previous courses, we used `SparkR` to access Spark
  * But we
  believe that the `sparklyr` package is a better long term bet.


# 4.2 A first Spark instance

## 4.2 A first Spark instance

The `sparklyr` package is on CRAN and installed in the usual way

```{r eval=FALSE}
install.packages("sparklyr")
```

To use the `sparklyr` package, you need a local copy of Spark.

```{r results="hide"}
library("sparklyr")
spark_available_versions()
```
## Installing Spark

Particular versions can then be installed

```{r eval=TRUE}
spark_install(version = "2.0.0")
```

Running this command will allow you to check that Spark is installed
correctly.

```{r eval=TRUE}
spark_installed_versions()
```

## Local & Remove clusters

You can connect to both local instances of Spark as well as remote
clusters. To illustrate the Spark, we'll connect to a local
instance

```{r}
# Typically, you'll want to connect to a remote instance.
sc = spark_connect(master = "local")
```

The returned Spark connection, `sc`, provides a remote `dplyr`
data source to (in this case) a local Spark cluster.


## Reading data

Typically you don't read in CSV files or convert existing data frames into
Spark; 

> If you could get objects into R in the first place why are you using
> Spark? 

## Example

You can copy R data frames into Spark using the `dplyr::copy_to`
function

```{r eval=TRUE, results="hide", message=FALSE}
library("dplyr")
data(movies, package="ggplot2movies")
movies_tbl = copy_to(sc, movies)
```

## tlb objects

The object `movies_tlb` just looks like a standard 
`dplyr` object. 

```{r echo=-1, results="hide"}
options(width=55)
movies_tbl
```

Notice that we are using lazy evaluation; we don't know how many rows
there are in this data set.

```{r eval=FALSE}
# Doesn't work
tail(movies_tbl)
```

## All tables


```{r}
src_tbls(sc)
```

Typically you would have more than a single table.


## Resilient distributed datasets (RDD)

  * The core feature of Spark is the resilient distributed dataset (RDD). 
  * An RDD is an abstraction that helps us deal with big data. 
  * An RDD is a distributed collection of elements (including data and functions). 
  * In Spark everything we do revolves around RDDs. 
  * Typically, we may want to create, transform or operate on the distributed data set.
  * Spark automatically handles how the data is distributed across your computer/cluster and parallelises operations where possible.

## Using `dplyr`

http://spark.rstudio.com/dplyr.html

## Using `dplyr`

  * The major benefit of using \cc{sparklyr} over \cc{SparkR} is that \cc{dplyr}
just slots into the work flow. 

```{r}
summarise(movies_tbl, 
          count = n(), 
          no_of_action = sum(action), 
          no_of_animation = sum(animation))
```

## group_by

```{r}
by_action = group_by(movies_tbl, action)
summarise(by_action, 
          mean = mean(rating), 
          sd = sd(rating)) 
```

as before.

## Using SQL

  * Standard SQL queries can also be passed to the cluster. 
  * The spark_connection object (`sc`) implements a DBI interface for Spark.
  * This means you can directly use `dbGetQuery` to execute SQL commands. 
  * The returned object is an R data frame
```{r}
library("DBI")
dbGetQuery(sc, "SELECT * FROM movies LIMIT 3")
```


## Machine learning/Statistcal algorithms

http://spark.rstudio.com/mllib.html


## Machine learning/Statistcal algorithms

  * The main reason for using Spark are the machine learning algorithms.
  * Most, __but not all!__ of this functionality is avalible within
`sparklyr`. 
  * As with the other functions, we access the machine learning
algorithms by passing a `spark_connection` object.

## Machine learning/Statistcal algorithms

In this example, we'll perform logistic regression to determine how a movies
classification (`Action`, `Comedy`, etc) affects it's overall 
movie rating.

```{r eval=FALSE}
m = movies_tbl %>%
  mutate(good = as.numeric(rating > mean(rating))) %>%
  ml_logistic_regression(good ~ Action + Animation + 
                             Comedy + Drama + Documentary + 
                             Romance + Short)
```

## Machine learning/Statistcal algorithms

The object `m` contains the relevant regression  cofficients

```{r eval=FALSE}
m
```


## Prediction

```{r}
movie_types = as.data.frame(diag(7))
colnames(movie_types) = c("Action", "Animation", "Comedy",
                          "Drama", "Documentary", "Romance",
                          "Short")
```

Then we copy the data frame to our Spark instance

```{r}
movie_types_tbl = copy_to(sc, movie_types, overwrite=TRUE)
```

## Predict

```{r eval=FALSE}
predict(m, movie_types_tbl, type="response")
```













